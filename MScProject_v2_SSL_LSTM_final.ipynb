{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiqYrhgnwCdX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um6BNCvrwPP3"
      },
      "outputs": [],
      "source": [
        "dataSetPath = []\n",
        "drive.mount('/content/drive')\n",
        "save_path = \"/content/drive/MyDrive/\"\n",
        "train_file_path = os.path.join(save_path, \"train_paths.txt\")\n",
        "test_file_path = os.path.join(save_path, \"test_paths.txt\")\n",
        "train_paths = []\n",
        "test_paths = []\n",
        "# Read the train paths from the file\n",
        "with open(train_file_path, \"r\") as train_file:\n",
        "    for line in train_file:\n",
        "        train_paths.append(line.strip())  # Remove newline characters\n",
        "\n",
        "# Read the test paths from the file\n",
        "with open(test_file_path, \"r\") as test_file:\n",
        "    for line in test_file:\n",
        "        test_paths.append(line.strip())  # Remove newline characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYlSwyI4thF-"
      },
      "outputs": [],
      "source": [
        "print(len(train_paths))\n",
        "print(len(test_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XlKAuPszfbP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09izP6anzo1X"
      },
      "outputs": [],
      "source": [
        "class WavDataset(Dataset):\n",
        "    def __init__(self, file_list, processor, max_length=10000):\n",
        "        self.file_list = file_list\n",
        "        self.max_length = max_length\n",
        "        self.processor = processor\n",
        "        self.transform = torchaudio.transforms.Resample(orig_freq=44100, new_freq=16000)  # 目标采样率为16000Hz\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_list[idx]\n",
        "        audio_input, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            audio_input = resampler(audio_input)\n",
        "\n",
        "        # if waveform.shape[1] > self.max_length:\n",
        "        #     waveform = waveform[:, :self.max_length]\n",
        "        #     #print('截取长度')\n",
        "        audio_label = file_path.split('/')[-1].split('_')[1][0]\n",
        "        if audio_label == 'C':\n",
        "            label = 0\n",
        "        elif audio_label == 'P':\n",
        "            label = 1\n",
        "        else:\n",
        "            label = audio_label\n",
        "        input_values = self.processor(audio_input.squeeze(0).numpy(), sampling_rate=16000, return_tensors=\"pt\").input_values\n",
        "        input_values = input_values.squeeze(0)\n",
        "\n",
        "        if input_values.size(0) < 10000:\n",
        "            pad_length = 10000 - input_values.size(0)\n",
        "            input_values = torch.nn.functional.pad(input_values, (0, pad_length), mode='constant', value=0)\n",
        "        return {'audioinfo': input_values, 'label': label}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3CXACX2c-y3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj0hWBhvc3-N"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_name = \"facebook/wav2vec2-base-960h\"\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "wav2vec2_model = Wav2Vec2Model.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5MMB8et5zfe"
      },
      "outputs": [],
      "source": [
        "train_dataset = WavDataset(train_paths,processor)\n",
        "test_dataset = WavDataset(test_paths,processor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
        "val_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQaHRGM8zUB6"
      },
      "outputs": [],
      "source": [
        "num = 0\n",
        "for batch in val_loader:\n",
        "    num = num + 1\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0tpA-Xf-TU6"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, base_model, hidden_size=128, num_layers=2, dropout_rate=0.1, pooling_type='min'):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.wav2vec2 = base_model\n",
        "        self.pooling_type = pooling_type\n",
        "\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.wav2vec2.config.hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_rate if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        with torch.no_grad():\n",
        "            features = self.wav2vec2(input_values).last_hidden_state\n",
        "\n",
        "        lstm_output, _ = self.lstm(features)\n",
        "\n",
        "\n",
        "        if self.pooling_type == 'mean':\n",
        "            pooled_output = torch.mean(lstm_output, dim=1)\n",
        "        elif self.pooling_type == 'max':\n",
        "            pooled_output, _ = torch.max(lstm_output, dim=1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported pooling type: {self.pooling_type}\")\n",
        "\n",
        "        # pooled_output = self.layer_norm(pooled_output)\n",
        "        # pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "classifier_model = LSTMClassifier(wav2vec2_model, pooling_type='mean')\n",
        "total_params = sum(p.numel() for p in classifier_model.parameters())\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "\n",
        "\n",
        "total_size = sum(p.numel() * p.element_size() for p in classifier_model.parameters())\n",
        "total_size_mb = total_size / (1024 ** 2)  # 转换为MB\n",
        "print(f\"Model Size: {total_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score, precision_score\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "save_path = \"/content/drive/MyDrive/Pretrained\"\n",
        "model_weights_path = os.path.join(save_path, 'Wav2vec-final.pth')\n",
        "\n",
        "#Ensure the model architecture is defined before loading\n",
        "classifier_model.load_state_dict(torch.load(model_weights_path))\n",
        "\n",
        "classifier_model.to(device)\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "correct = 0\n",
        "# Inside your loop\n",
        "with torch.no_grad():\n",
        "    for data in tqdm(val_loader):\n",
        "        input_values = data['audioinfo'].float().to(device)\n",
        "        labels = data['label'].long().to(device)\n",
        "\n",
        "        outputs = classifier_model(input_values)\n",
        "        loss = criterion(outputs, labels)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
        "\n",
        "\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "\n",
        "\n",
        "plt.title('End-to-end Model Confusion Matrix')\n",
        "plt.xticks(ticks=[0.5, 1.5], labels=['Negative', 'Positive'])\n",
        "plt.yticks(ticks=[0.5, 1.5], labels=['Negative', 'Positive'], rotation=0)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "# Calculate accuracy, recall, and F1 score\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "recall = recall_score(all_labels, all_preds, average='macro')  # Use 'micro' or 'weighted' if needed\n",
        "f1 = f1_score(all_labels, all_preds, average='macro')  # Use 'micro' or 'weighted' if needed\n",
        "precision = precision_score(all_labels, all_preds, average='macro')  # Use 'micro' or 'weighted' if needed\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "2R66nf3I_hV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMwHw3TN-knG"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier_model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize lists to store metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for data in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "            input_values = data['audioinfo'].float().to(device)\n",
        "            labels = data['label'].long().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_values)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * input_values.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)  # Store train loss\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in tqdm(val_loader, desc=f\"Testing Epoch {epoch+1}\"):\n",
        "                input_values = data['audioinfo'].float().to(device)\n",
        "                labels = data['label'].long().to(device)\n",
        "\n",
        "                outputs = model(input_values)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * input_values.size(0)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_accuracy = correct / len(val_loader.dataset)\n",
        "\n",
        "        val_losses.append(val_loss)  # Store validation loss\n",
        "        val_accuracies.append(val_accuracy)  # Store validation accuracy\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "        print(f\"Training Loss: {train_loss:.4f}\")\n",
        "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "        save_path = \"/content/drive/MyDrive/Pretrained\"\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        torch.save(classifier_model.state_dict(), os.path.join(save_path, f'Wav2vec-final.pth'))\n",
        "        if val_accuracy > 0.795:\n",
        "          print(\"Finish training\")\n",
        "          break\n",
        "\n",
        "\n",
        "    # Plotting the metrics\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', color='orange')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "save_path = \"/content/drive/MyDrive/Pretrained\"\n",
        "model_weights_path = os.path.join(save_path, 'Wav2vec-final.pth')\n",
        "\n",
        "#Ensure the model architecture is defined before loading\n",
        "classifier_model.load_state_dict(torch.load(model_weights_path))\n",
        "\n",
        "train_model(classifier_model, train_loader, val_loader, criterion, optimizer, num_epochs=50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}